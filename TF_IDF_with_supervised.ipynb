{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X3sydFqZ2p7",
        "outputId": "16e15e6d-f216-4e6e-c6e5-85c2b0ac7831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=d484274f1905a708648eb129ed5e6c173f231a2821843ca31961693d8057e353\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XAaGJqWYc6A",
        "outputId": "63078031-83d8-4a8d-c980-57bb9af102fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average recall is: \n",
            "0.6696207223501107\n",
            "average Precision is: \n",
            "0.7087207921579842\n",
            "f-measure average: \n",
            "0.6886161721084617\n",
            "for ROUGE 2\n",
            "ROUGE-2 average Precision is: \n",
            "0.5090335884244451\n",
            "AROUGE-2 average recall is: \n",
            "0.4792975590467836\n",
            "ROUGE-2 f-measure average: \n",
            "0.49371823812071874\n"
          ]
        }
      ],
      "source": [
        "# This Python file uses the following encoding: utf-8\n",
        "\n",
        "import nltk\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import math\n",
        "import codecs\n",
        "import pandas as pd\n",
        "import re\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "\n",
        "def ROUGE2_pre(g, h):\n",
        "\n",
        "    ac_words1 = g.split(' ')\n",
        "    pro_words1 = h.split(' ')\n",
        "\n",
        "\n",
        "    ac_words = list(nltk.bigrams(ac_words1))\n",
        "    pro_words = list(nltk.bigrams(pro_words1))\n",
        "\n",
        "\n",
        "    count = 0\n",
        "    if len(ac_words) > len(pro_words):\n",
        "        for x in pro_words:\n",
        "            for k in ac_words:\n",
        "               if x == k:\n",
        "                    count= count + 1\n",
        "                    break\n",
        "    elif len(pro_words)>len(ac_words):\n",
        "        for x in ac_words:\n",
        "            for k in pro_words:\n",
        "                if x == k:\n",
        "                    count = count + 1\n",
        "                    break\n",
        "\n",
        "    pre_rouge_2 = count / len(pro_words)\n",
        "\n",
        "    return pre_rouge_2\n",
        "\n",
        "\n",
        "\n",
        "def ROUGE2_re(g, h):\n",
        "\n",
        "\n",
        "    ac_words1 = g.split(' ')\n",
        "    pro_words1 = h.split(' ')\n",
        "\n",
        "\n",
        "    ac_words = list(nltk.bigrams(ac_words1))\n",
        "    pro_words = list(nltk.bigrams(pro_words1))\n",
        "\n",
        "    count = 0\n",
        "    if len(ac_words) > len(pro_words):\n",
        "        for x in pro_words:\n",
        "            for k in ac_words:\n",
        "                if x == k:\n",
        "                    count = count + 1\n",
        "                    break\n",
        "    elif len(pro_words) > len(ac_words):\n",
        "        for x in ac_words:\n",
        "            for k in pro_words:\n",
        "                if x == k:\n",
        "                    count = count + 1\n",
        "                    break\n",
        "\n",
        "    # Storing the final accuracy of Rouge-1\n",
        "    if(len(ac_words)==0):\n",
        "      re_rouge_2 = count / 1\n",
        "    else:\n",
        "      re_rouge_2 = count / len(ac_words)\n",
        "\n",
        "    return re_rouge_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ROUGE1_pre(g, h):\n",
        "\n",
        "    ac_words = g.split(' ')\n",
        "    pro_words = h.split(' ')\n",
        "\n",
        "\n",
        "    count = 0\n",
        "    if len(ac_words) > len(pro_words):\n",
        "        for x in pro_words:\n",
        "            for k in ac_words:\n",
        "               if x == k:\n",
        "                    count= count + 1\n",
        "                    break\n",
        "    elif len(pro_words)>len(ac_words):\n",
        "        for x in ac_words:\n",
        "            for k in pro_words:\n",
        "                if x == k:\n",
        "                    count = count + 1\n",
        "                    break\n",
        "\n",
        "\n",
        "    pre_rouge_1 = count / len(pro_words)\n",
        "\n",
        "    return pre_rouge_1\n",
        "\n",
        "\n",
        "def ROUGE1_re(g, h):\n",
        "\n",
        "    ac_words = g.split(' ')\n",
        "    pro_words = h.split(' ')\n",
        "\n",
        "    count = 0\n",
        "    if len(ac_words) > len(pro_words):\n",
        "        for x in pro_words:\n",
        "            for k in ac_words:\n",
        "                if x == k:\n",
        "                    count = count + 1\n",
        "                    break\n",
        "    elif len(pro_words) > len(ac_words):\n",
        "        for x in ac_words:\n",
        "            for k in pro_words:\n",
        "                if x == k:\n",
        "                    count = count + 1\n",
        "                    break\n",
        "\n",
        "    if(len(ac_words)==0):\n",
        "      re_rouge_1 = count / 1\n",
        "    else:\n",
        "      re_rouge_1 = count / len(ac_words)\n",
        "\n",
        "    return re_rouge_1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def stopwordremoval(text):\n",
        "\n",
        "    dfsw = pd.read_csv('Stop_words.csv')\n",
        "    words = dfsw.s_words.values.tolist()\n",
        "    tokens = text.split(' ')\n",
        "\n",
        "    filtered_sentence = []\n",
        "    for w in tokens:\n",
        "        if w not in words:\n",
        "            filtered_sentence.append(w)\n",
        "    new_text = ' '.join(filtered_sentence)\n",
        "    return new_text\n",
        "\n",
        "\n",
        "def sen_maker(text):\n",
        "    sen = text.replace('۔', '%%%')\n",
        "    sen = sen.replace('\\r\\n', ' ')\n",
        "    sen = sen.split('%%%')\n",
        "    return sen\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def createfrequencytable(text_string) -> dict:\n",
        "    dfsw = pd.read_csv('Stop_words.csv')\n",
        "    stopWords = dfsw.s_words.values.tolist()\n",
        "    words = text_string.split(' ')\n",
        "    ps = PorterStemmer()\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = str(word)\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "    return freqTable\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def scoresentences(sentences, freqTable) -> dict:\n",
        "    sentenceValue = dict()\n",
        "    for sentence in sentences:\n",
        "        word_count_in_sentence = (len(sentence.split(' ')))\n",
        "        for wordValue in freqTable:\n",
        "            if wordValue in sentence.lower():\n",
        "                if sentence[:10] in sentenceValue:\n",
        "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
        "                else:\n",
        "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
        "    sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] // word_count_in_sentence\n",
        "    return sentenceValue\n",
        "\n",
        "\n",
        "\n",
        "def findaverage_score(sentenceValue) -> int:\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "    average = int(sumValues / len(sentenceValue))\n",
        "    return average\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def has_numbers(inputString):\n",
        "    return any(char.isdigit() for char in inputString)\n",
        "\n",
        "\n",
        "def noun(text):\n",
        "    dfPOS = pd.read_csv('POS_Nouns.csv')\n",
        "    term = dfPOS.Term.values.tolist()\n",
        "    text = stopwordremoval(text)\n",
        "    a = re.split(' ', text)\n",
        "    j = 0\n",
        "    for w in a:\n",
        "        if w in term:\n",
        "            return 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def word_prob(sentence,ftable):\n",
        "    sentence = stopwordremoval(sentence)\n",
        "    words_sen = sentence.split(' ')\n",
        "    max_term = max(ftable, key=ftable.get)\n",
        "    max_freq = ftable[max_term]\n",
        "    w_prob = 0\n",
        "    if max_freq == 0:\n",
        "        max_freq = 1\n",
        "    for word in words_sen:\n",
        "        if ftable.get(word):\n",
        "            wp = ftable[word] / max_freq\n",
        "            if wp > 0.5:\n",
        "                w_prob = 1\n",
        "    return w_prob\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _generate_summary(sentences, sentenceValue, threshold, ft):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "    for sentence in sentences:\n",
        "        if ((sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] > (threshold)) or (has_numbers(sentence)) or ((word_prob(sentence,ft)) and (noun(sentence))) ):\n",
        "            summary += \"۔\" + sentence\n",
        "            sentence_count += 1\n",
        "    return summary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_summary(text1):\n",
        "    text = text1\n",
        "    ft = createfrequencytable(text)\n",
        "    sentences = sen_maker(text)  # NLTK function\n",
        "    total_documents = len(sentences)\n",
        "\n",
        "    sentence_val = scoresentences(sentences, ft)\n",
        "\n",
        "    thresh = findaverage_score(sentence_val)\n",
        "\n",
        "    summary = _generate_summary(sentences, sentence_val, 2 * thresh, ft)\n",
        "    return summary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('Urdu_7_Books.csv')\n",
        "\n",
        "\n",
        "X = df.Text\n",
        "y = df.Extractive_summaries\n",
        "    #df.Khulasay\n",
        "z = df.Markazi_Khayal\n",
        "\n",
        "j = 0\n",
        "precount = 1\n",
        "presumscore = 0\n",
        "preavg = 0\n",
        "\n",
        "resumscore = 0\n",
        "reavg = 0\n",
        "\n",
        "f_score = 0\n",
        "f_sums = 0\n",
        "f_avg = 0\n",
        "\n",
        "\n",
        "\n",
        "presumscore2 = 0\n",
        "preavg2 = 0\n",
        "\n",
        "resumscore2 = 0\n",
        "reavg2 = 0\n",
        "\n",
        "f_score2 = 0\n",
        "f_sums2 = 0\n",
        "f_avg2 = 0\n",
        "\n",
        "for x in X:\n",
        "    XC = create_summary(str(' '+X[j]+' '))\n",
        "\n",
        "\n",
        "    #======= ROUGE-1 Scores=============================\n",
        "\n",
        "\n",
        "    precision = ROUGE1_pre(XC, y[j])\n",
        "    recall = ROUGE1_re(XC, y[j])\n",
        "\n",
        "    presumscore = presumscore + precision\n",
        "    resumscore = resumscore + recall\n",
        "\n",
        "    preavg = presumscore / precount\n",
        "    reavg = resumscore / precount\n",
        "\n",
        "    #==========ROUGE-2==========================================\n",
        "\n",
        "\n",
        "\n",
        "    precision2 = ROUGE2_pre(XC, y[j])\n",
        "    recall2 = ROUGE2_re(XC, y[j])\n",
        "    #print(XC)\n",
        "\n",
        "    presumscore2 = presumscore2 + precision2\n",
        "    resumscore2 = resumscore2 + recall2\n",
        "\n",
        "    preavg2 = presumscore2 / precount\n",
        "    reavg2 = resumscore2 / precount\n",
        "\n",
        "    precount = precount + 1\n",
        "    j = j + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "f_score = 2 * (preavg * reavg) / (preavg + reavg)\n",
        "\n",
        "f_score2 = 2 * (preavg2 * reavg2) / (preavg2 + reavg2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"average recall is: \")\n",
        "print(reavg)\n",
        "\n",
        "print(\"average Precision is: \")\n",
        "print(preavg)\n",
        "\n",
        "print(\"f-measure average: \")\n",
        "print(f_score)\n",
        "\n",
        "print(\"for ROUGE 2\")\n",
        "\n",
        "print(\"ROUGE-2 average Precision is: \")\n",
        "print(preavg2)\n",
        "\n",
        "\n",
        "print(\"AROUGE-2 average recall is: \")\n",
        "print(reavg2)\n",
        "\n",
        "print(\"ROUGE-2 f-measure average: \")\n",
        "print(f_score2)\n",
        "\n",
        "\n"
      ]
    }
  ]
}